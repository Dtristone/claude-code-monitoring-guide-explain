# Local Offline OTEL Metrics Analysis

This guide explains how to capture and analyze Claude Code OTEL metrics **locally without requiring external services** (no Docker, no OTEL Collector, no Prometheus, no internet access).

> ðŸ“– **Official Documentation**: For the complete list of available metrics and telemetry configuration, see the [Claude Code Monitoring Usage Guide](https://docs.anthropic.com/en/docs/claude-code/monitoring-usage).

This is ideal for:
- Air-gapped environments without internet access
- Standalone developer workstations
- Quick local analysis without infrastructure setup
- Environments where Docker is not available

## Table of Contents

1. [Overview](#overview)
2. [Available Metrics](#available-metrics)
3. [Quick Start](#quick-start)
4. [Capturing Metrics Locally](#capturing-metrics-locally)
5. [Local Analysis Scripts](#local-analysis-scripts)
6. [Database Schema](#database-schema)
7. [Generating Reports](#generating-reports)
8. [Timeline Visualization](#timeline-visualization)
9. [Combining with Session JSONL Files](#combining-with-session-jsonl-files)

---

## Overview

### Architecture (Offline Mode)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚     â”‚                     â”‚     â”‚                 â”‚     â”‚                 â”‚
â”‚   Claude Code   â”‚â”€â”€â”€â”€â–¶â”‚  Console Output     â”‚â”€â”€â”€â”€â–¶â”‚  Local Script   â”‚â”€â”€â”€â”€â–¶â”‚  SQLite DB +    â”‚
â”‚   (Your IDE)    â”‚     â”‚  (OTEL Metrics)     â”‚     â”‚  (Parser)       â”‚     â”‚  Reports        â”‚
â”‚                 â”‚     â”‚                     â”‚     â”‚                 â”‚     â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                         â”‚                         â”‚                       â”‚
        â”‚   OTEL_METRICS_EXPORTER â”‚   Redirect to file      â”‚   Parse & Store       â”‚
        â”‚         =console        â”‚                         â”‚                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Differences from Server-Based Setup

| Aspect | Server-Based (Docker) | Offline (Local) |
|--------|----------------------|-----------------|
| Infrastructure | Docker, OTEL Collector, Prometheus, Grafana | None - just Python scripts |
| Network | Requires localhost ports 4317, 8889, 9090, 3000 | No network required |
| Data Storage | Prometheus time-series DB | Local SQLite file |
| Querying | PromQL | SQL queries |
| Visualization | Grafana dashboards | Text reports, CSV export |
| Internet | May need for Docker images | Completely offline |

---

## Available Metrics

Claude Code exports the following OpenTelemetry metrics. For the most up-to-date list and detailed documentation, see the [official monitoring guide](https://docs.anthropic.com/en/docs/claude-code/monitoring-usage).

### Core Metrics

| Metric Name | Description | Unit | Labels |
|-------------|-------------|------|--------|
| `claude_code.token.usage` | Number of tokens used | tokens | `user_id`, `session_id`, `model`, `type` |
| `claude_code.cost.usage` | Cost of the Claude Code session | USD | `user_id`, `session_id`, `model` |
| `claude_code.session.count` | Count of CLI sessions started | count | `user_id` |
| `claude_code.active_time.total` | Total active time in seconds | seconds | `user_id`, `session_id` |

### Productivity Metrics

| Metric Name | Description | Unit | Labels |
|-------------|-------------|------|--------|
| `claude_code.lines_of_code.count` | Count of lines of code modified | count | `user_id`, `session_id`, `type` |
| `claude_code.commit.count` | Number of git commits created | count | `user_id`, `session_id` |
| `claude_code.pull_request.count` | Number of pull requests created | count | `user_id`, `session_id` |
| `claude_code.code_edit_tool.decision` | Code editing tool permission decisions | count | `user_id`, `session_id`, `decision` |

### Token Type Labels

The `type` label for `token.usage` metric can have these values:

| Type | Description |
|------|-------------|
| `input` | Regular input tokens sent to the model |
| `output` | Output tokens generated by the model |
| `cacheRead` | Tokens retrieved from the prefix cache (cache hits) |
| `cacheCreation` | Tokens stored in the prefix cache (new cache entries) |

### Metric Labels Reference

| Label | Description | Example |
|-------|-------------|---------|
| `user_id` | Hashed user identifier | `9d5e2d20425102ac6bf66e04353a484242a49a203ff0527e6ec1e0fcfc996fc6` |
| `session_id` | Unique session identifier | `14f70ee9-4f03-4def-a06b-4c1689198192` |
| `model` | AI model used | `claude-4-sonnet`, `claude-3-5-haiku` |
| `type` | Token type or activity type | `input`, `output`, `cacheRead`, `cacheCreation` |
| `decision` | Tool permission decision | `accept`, `reject` |

---

## Quick Start

### Step 1: Configure Claude Code for Console Output

Set these environment variables to export metrics to console:

```bash
export CLAUDE_CODE_ENABLE_TELEMETRY=1
export OTEL_METRICS_EXPORTER=console
export OTEL_METRIC_EXPORT_INTERVAL=5000
```

Or add to `~/.claude.json`:

```json
{
  "env": {
    "CLAUDE_CODE_ENABLE_TELEMETRY": "1",
    "OTEL_METRICS_EXPORTER": "console",
    "OTEL_METRIC_EXPORT_INTERVAL": "5000"
  }
}
```

### Step 2: Capture Metrics to File

Run Claude Code and redirect output to a file:

```bash
# Run Claude Code and capture all output
claude 2>&1 | tee -a ~/claude_metrics.log
```

Or run a specific command:

```bash
claude -p "your prompt here" 2>&1 | tee -a ~/claude_metrics.log
```

### Step 3: Parse and Analyze

Use the provided Python scripts to parse and analyze:

```bash
# Parse metrics and store in SQLite
python3 scripts/parse_otel_metrics.py ~/claude_metrics.log

# Generate report
python3 scripts/generate_local_report.py

# View timeline
python3 scripts/generate_timeline.py
```

---

## Capturing Metrics Locally

### Console Output Format

When `OTEL_METRICS_EXPORTER=console` is set, Claude Code outputs metrics in this format:

```
=== Resource Attributes ===
{ 'service.name': 'claude-code', 'service.version': '2.1.25' }
===========================

{
  descriptor: {
    name: 'claude_code.token.usage',
    type: 'COUNTER',
    description: 'Number of tokens used',
    unit: 'tokens',
    valueType: 1
  },
  dataPointType: 3,
  dataPoints: [
    {
      attributes: {
        'user.id': 'abc123...',
        'session.id': '14f70ee9-4f03-4def-a06b-4c1689198192',
        'model': 'claude-4-sonnet',
        'type': 'input'
      },
      value: 750
    }
  ]
}
```

### Recommended Capture Setup

Create a capture script `~/capture_claude_metrics.sh`:

```bash
#!/bin/bash
# capture_claude_metrics.sh - Capture Claude Code metrics to file

LOG_DIR="${HOME}/.claude-metrics"
LOG_FILE="${LOG_DIR}/metrics_$(date +%Y%m%d).log"

mkdir -p "$LOG_DIR"

echo "=== Session Started: $(date -Iseconds) ===" >> "$LOG_FILE"

# Set environment
export CLAUDE_CODE_ENABLE_TELEMETRY=1
export OTEL_METRICS_EXPORTER=console
export OTEL_METRIC_EXPORT_INTERVAL=5000

# Run Claude Code with output capture
claude "$@" 2>&1 | tee -a "$LOG_FILE"

echo "=== Session Ended: $(date -Iseconds) ===" >> "$LOG_FILE"
```

Make it executable and use instead of `claude`:

```bash
chmod +x ~/capture_claude_metrics.sh
~/capture_claude_metrics.sh -p "your prompt"
```

---

## Local Analysis Scripts

### Script 1: parse_otel_metrics.py

Save this as `scripts/parse_otel_metrics.py`:

```python
#!/usr/bin/env python3
"""
Parse OTEL console output and store metrics in SQLite database.

Usage:
    python parse_otel_metrics.py <log_file> [--db <database_file>]
    
Example:
    python parse_otel_metrics.py ~/claude_metrics.log --db ~/claude_metrics.db
"""

import re
import json
import sqlite3
import argparse
from datetime import datetime
from pathlib import Path


def create_database(db_path):
    """Create SQLite database with schema for OTEL metrics."""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # Create tables
    cursor.executescript('''
        -- Sessions table
        CREATE TABLE IF NOT EXISTS sessions (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            session_id TEXT UNIQUE,
            user_id TEXT,
            model TEXT,
            started_at TIMESTAMP,
            ended_at TIMESTAMP
        );
        
        -- Token usage table
        CREATE TABLE IF NOT EXISTS token_usage (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            session_id TEXT,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            model TEXT,
            token_type TEXT,  -- input, output, cacheRead, cacheCreation
            value INTEGER,
            FOREIGN KEY (session_id) REFERENCES sessions(session_id)
        );
        
        -- Cost usage table
        CREATE TABLE IF NOT EXISTS cost_usage (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            session_id TEXT,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            model TEXT,
            value REAL,
            FOREIGN KEY (session_id) REFERENCES sessions(session_id)
        );
        
        -- Active time table
        CREATE TABLE IF NOT EXISTS active_time (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            session_id TEXT,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            value REAL,  -- seconds
            FOREIGN KEY (session_id) REFERENCES sessions(session_id)
        );
        
        -- Code activity table
        CREATE TABLE IF NOT EXISTS code_activity (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            session_id TEXT,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            activity_type TEXT,  -- commit, pull_request, lines_of_code
            value INTEGER,
            FOREIGN KEY (session_id) REFERENCES sessions(session_id)
        );
        
        -- Raw metrics table (for debugging)
        CREATE TABLE IF NOT EXISTS raw_metrics (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            metric_name TEXT,
            raw_json TEXT
        );
        
        -- Create indexes
        CREATE INDEX IF NOT EXISTS idx_token_session ON token_usage(session_id);
        CREATE INDEX IF NOT EXISTS idx_token_type ON token_usage(token_type);
        CREATE INDEX IF NOT EXISTS idx_cost_session ON cost_usage(session_id);
        CREATE INDEX IF NOT EXISTS idx_activity_session ON code_activity(session_id);
    ''')
    
    conn.commit()
    return conn


def parse_metric_block(block):
    """Parse a single metric block from console output."""
    try:
        # Clean up JavaScript-style output to valid JSON
        # Replace single quotes with double quotes
        cleaned = block.replace("'", '"')
        # Handle unquoted keys
        cleaned = re.sub(r'(\w+):', r'"\1":', cleaned)
        # Handle trailing commas
        cleaned = re.sub(r',(\s*[}\]])', r'\1', cleaned)
        
        return json.loads(cleaned)
    except json.JSONDecodeError:
        return None


def extract_metrics_from_log(log_content):
    """Extract all metric blocks from log file content."""
    metrics = []
    
    # Pattern to match metric blocks
    # Look for blocks starting with { and containing 'descriptor'
    pattern = r'\{[^{}]*descriptor[^{}]*\{[^{}]*\}[^{}]*dataPoints[^{}]*\[[^\[\]]*\{[^{}]*\}[^\[\]]*\][^{}]*\}'
    
    # Simpler approach: split by metric markers and parse
    blocks = re.split(r'(?=\{\s*descriptor:)', log_content)
    
    for block in blocks:
        if 'descriptor:' in block:
            # Find the complete JSON object
            brace_count = 0
            start = block.find('{')
            if start == -1:
                continue
                
            end = start
            for i, char in enumerate(block[start:], start):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        end = i + 1
                        break
            
            metric_str = block[start:end]
            parsed = parse_metric_block(metric_str)
            if parsed:
                metrics.append(parsed)
    
    return metrics


def store_metric(conn, metric, timestamp=None):
    """Store a parsed metric in the database."""
    if not metric or 'descriptor' not in metric:
        return
    
    cursor = conn.cursor()
    timestamp = timestamp or datetime.now().isoformat()
    
    descriptor = metric.get('descriptor', {})
    metric_name = descriptor.get('name', '')
    data_points = metric.get('dataPoints', [])
    
    # Store raw metric for debugging
    cursor.execute(
        'INSERT INTO raw_metrics (timestamp, metric_name, raw_json) VALUES (?, ?, ?)',
        (timestamp, metric_name, json.dumps(metric))
    )
    
    for dp in data_points:
        attrs = dp.get('attributes', {})
        session_id = attrs.get('session.id', attrs.get('session_id', 'unknown'))
        user_id = attrs.get('user.id', attrs.get('user_id', 'unknown'))
        model = attrs.get('model', 'unknown')
        value = dp.get('value', 0)
        
        # Ensure session exists
        cursor.execute('''
            INSERT OR IGNORE INTO sessions (session_id, user_id, model, started_at)
            VALUES (?, ?, ?, ?)
        ''', (session_id, user_id, model, timestamp))
        
        # Store based on metric type
        if 'token' in metric_name.lower():
            token_type = attrs.get('type', 'unknown')
            cursor.execute('''
                INSERT INTO token_usage (session_id, timestamp, model, token_type, value)
                VALUES (?, ?, ?, ?, ?)
            ''', (session_id, timestamp, model, token_type, value))
            
        elif 'cost' in metric_name.lower():
            cursor.execute('''
                INSERT INTO cost_usage (session_id, timestamp, model, value)
                VALUES (?, ?, ?, ?)
            ''', (session_id, timestamp, model, value))
            
        elif 'active_time' in metric_name.lower():
            cursor.execute('''
                INSERT INTO active_time (session_id, timestamp, value)
                VALUES (?, ?, ?)
            ''', (session_id, timestamp, value))
            
        elif any(x in metric_name.lower() for x in ['commit', 'pull_request', 'lines_of_code']):
            activity_type = metric_name.split('.')[-1] if '.' in metric_name else metric_name
            cursor.execute('''
                INSERT INTO code_activity (session_id, timestamp, activity_type, value)
                VALUES (?, ?, ?, ?)
            ''', (session_id, timestamp, activity_type, value))
    
    conn.commit()


def parse_log_file(log_path, db_path):
    """Parse a log file and store metrics in database."""
    print(f"Parsing log file: {log_path}")
    print(f"Database: {db_path}")
    
    with open(log_path, 'r') as f:
        content = f.read()
    
    conn = create_database(db_path)
    metrics = extract_metrics_from_log(content)
    
    print(f"Found {len(metrics)} metric blocks")
    
    for metric in metrics:
        store_metric(conn, metric)
    
    # Print summary
    cursor = conn.cursor()
    
    cursor.execute('SELECT COUNT(DISTINCT session_id) FROM sessions')
    session_count = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM token_usage')
    token_count = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM cost_usage')
    cost_count = cursor.fetchone()[0]
    
    print(f"\nSummary:")
    print(f"  Sessions: {session_count}")
    print(f"  Token records: {token_count}")
    print(f"  Cost records: {cost_count}")
    
    conn.close()
    print(f"\nDatabase saved to: {db_path}")


def main():
    parser = argparse.ArgumentParser(
        description='Parse OTEL console output and store in SQLite database'
    )
    parser.add_argument('log_file', help='Path to log file with OTEL console output')
    parser.add_argument('--db', default='~/.claude-metrics/metrics.db',
                        help='Path to SQLite database file')
    
    args = parser.parse_args()
    
    log_path = Path(args.log_file).expanduser()
    db_path = Path(args.db).expanduser()
    
    # Ensure directory exists
    db_path.parent.mkdir(parents=True, exist_ok=True)
    
    if not log_path.exists():
        print(f"Error: Log file not found: {log_path}")
        return 1
    
    parse_log_file(str(log_path), str(db_path))
    return 0


if __name__ == '__main__':
    exit(main())
```

### Script 2: generate_local_report.py

Save this as `scripts/generate_local_report.py`:

```python
#!/usr/bin/env python3
"""
Generate reports from local SQLite metrics database.

Usage:
    python generate_local_report.py [--db <database_file>] [--output <output_file>]
    
Example:
    python generate_local_report.py --db ~/claude_metrics.db --output report.md
"""

import sqlite3
import argparse
from datetime import datetime
from pathlib import Path


def get_summary_stats(cursor):
    """Get overall summary statistics."""
    stats = {}
    
    # Total sessions
    cursor.execute('SELECT COUNT(DISTINCT session_id) FROM sessions')
    stats['total_sessions'] = cursor.fetchone()[0]
    
    # Total tokens by type
    cursor.execute('''
        SELECT token_type, SUM(value) as total
        FROM token_usage
        GROUP BY token_type
    ''')
    stats['tokens_by_type'] = dict(cursor.fetchall())
    
    # Total cost
    cursor.execute('SELECT SUM(value) FROM cost_usage')
    result = cursor.fetchone()[0]
    stats['total_cost'] = result if result else 0
    
    # Total active time
    cursor.execute('SELECT SUM(value) FROM active_time')
    result = cursor.fetchone()[0]
    stats['total_active_time'] = result if result else 0
    
    # Model usage
    cursor.execute('''
        SELECT model, SUM(value) as total
        FROM token_usage
        GROUP BY model
    ''')
    stats['tokens_by_model'] = dict(cursor.fetchall())
    
    # Cost by model
    cursor.execute('''
        SELECT model, SUM(value) as total
        FROM cost_usage
        GROUP BY model
    ''')
    stats['cost_by_model'] = dict(cursor.fetchall())
    
    return stats


def get_session_details(cursor):
    """Get detailed session information."""
    cursor.execute('''
        SELECT 
            s.session_id,
            s.model,
            s.started_at,
            COALESCE(SUM(CASE WHEN t.token_type = 'input' THEN t.value ELSE 0 END), 0) as input_tokens,
            COALESCE(SUM(CASE WHEN t.token_type = 'output' THEN t.value ELSE 0 END), 0) as output_tokens,
            COALESCE(SUM(CASE WHEN t.token_type = 'cacheRead' THEN t.value ELSE 0 END), 0) as cache_read,
            COALESCE(SUM(CASE WHEN t.token_type = 'cacheCreation' THEN t.value ELSE 0 END), 0) as cache_creation,
            COALESCE((SELECT SUM(value) FROM cost_usage WHERE session_id = s.session_id), 0) as cost,
            COALESCE((SELECT SUM(value) FROM active_time WHERE session_id = s.session_id), 0) as active_seconds
        FROM sessions s
        LEFT JOIN token_usage t ON s.session_id = t.session_id
        GROUP BY s.session_id
        ORDER BY s.started_at DESC
    ''')
    
    columns = ['session_id', 'model', 'started_at', 'input_tokens', 'output_tokens', 
               'cache_read', 'cache_creation', 'cost', 'active_seconds']
    
    return [dict(zip(columns, row)) for row in cursor.fetchall()]


def calculate_cache_metrics(cursor):
    """Calculate cache efficiency metrics."""
    cursor.execute('''
        SELECT 
            SUM(CASE WHEN token_type = 'cacheRead' THEN value ELSE 0 END) as cache_read,
            SUM(CASE WHEN token_type = 'cacheCreation' THEN value ELSE 0 END) as cache_creation,
            SUM(CASE WHEN token_type = 'input' THEN value ELSE 0 END) as input
        FROM token_usage
    ''')
    
    row = cursor.fetchone()
    cache_read = row[0] or 0
    cache_creation = row[1] or 0
    input_tokens = row[2] or 0
    
    metrics = {
        'cache_read_tokens': cache_read,
        'cache_creation_tokens': cache_creation,
        'input_tokens': input_tokens,
        'cache_hit_ratio': cache_read / (cache_read + cache_creation) if (cache_read + cache_creation) > 0 else 0,
        'cache_efficiency': cache_read / (cache_read + input_tokens) if (cache_read + input_tokens) > 0 else 0,
        'cache_read_creation_ratio': cache_read / cache_creation if cache_creation > 0 else 0,
        # Estimated cost savings (cache reads cost ~10% of regular input)
        'estimated_savings': cache_read * 0.000003 * 0.9
    }
    
    return metrics


def generate_markdown_report(stats, sessions, cache_metrics):
    """Generate a markdown report."""
    report = []
    
    report.append("# Claude Code Local Metrics Report")
    report.append(f"\n**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # Summary section
    report.append("## Summary\n")
    report.append(f"- **Total Sessions**: {stats['total_sessions']}")
    report.append(f"- **Total Cost**: ${stats['total_cost']:.4f}")
    report.append(f"- **Total Active Time**: {stats['total_active_time']:.1f} seconds ({stats['total_active_time']/60:.1f} minutes)")
    
    # Token usage
    report.append("\n## Token Usage\n")
    report.append("| Type | Count |")
    report.append("|------|-------|")
    for token_type, count in stats['tokens_by_type'].items():
        report.append(f"| {token_type} | {count:,} |")
    
    total_tokens = sum(stats['tokens_by_type'].values())
    report.append(f"| **Total** | **{total_tokens:,}** |")
    
    # Cost by model
    if stats['cost_by_model']:
        report.append("\n## Cost by Model\n")
        report.append("| Model | Cost (USD) |")
        report.append("|-------|------------|")
        for model, cost in stats['cost_by_model'].items():
            report.append(f"| {model} | ${cost:.4f} |")
    
    # Cache metrics
    report.append("\n## Cache Efficiency\n")
    report.append(f"- **Cache Read Tokens**: {cache_metrics['cache_read_tokens']:,}")
    report.append(f"- **Cache Creation Tokens**: {cache_metrics['cache_creation_tokens']:,}")
    report.append(f"- **Cache Hit Ratio**: {cache_metrics['cache_hit_ratio']*100:.1f}%")
    report.append(f"- **Cache Efficiency**: {cache_metrics['cache_efficiency']*100:.1f}%")
    report.append(f"- **Cache Read:Creation Ratio**: {cache_metrics['cache_read_creation_ratio']:.1f}:1")
    report.append(f"- **Estimated Cost Savings**: ${cache_metrics['estimated_savings']:.4f}")
    
    # Session details
    report.append("\n## Session Details\n")
    report.append("| Session ID | Model | Input | Output | Cache Read | Cost |")
    report.append("|------------|-------|-------|--------|------------|------|")
    for session in sessions[:20]:  # Limit to 20 sessions
        session_short = session['session_id'][:8] + '...' if len(session['session_id']) > 12 else session['session_id']
        report.append(f"| {session_short} | {session['model']} | {session['input_tokens']:,} | {session['output_tokens']:,} | {session['cache_read']:,} | ${session['cost']:.4f} |")
    
    if len(sessions) > 20:
        report.append(f"\n*Showing 20 of {len(sessions)} sessions*")
    
    return '\n'.join(report)


def generate_report(db_path, output_path=None):
    """Generate a full report from the database."""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    stats = get_summary_stats(cursor)
    sessions = get_session_details(cursor)
    cache_metrics = calculate_cache_metrics(cursor)
    
    report = generate_markdown_report(stats, sessions, cache_metrics)
    
    if output_path:
        with open(output_path, 'w') as f:
            f.write(report)
        print(f"Report saved to: {output_path}")
    else:
        print(report)
    
    conn.close()


def main():
    parser = argparse.ArgumentParser(
        description='Generate reports from local SQLite metrics database'
    )
    parser.add_argument('--db', default='~/.claude-metrics/metrics.db',
                        help='Path to SQLite database file')
    parser.add_argument('--output', '-o', help='Output file path (default: print to stdout)')
    
    args = parser.parse_args()
    
    db_path = Path(args.db).expanduser()
    output_path = Path(args.output).expanduser() if args.output else None
    
    if not db_path.exists():
        print(f"Error: Database not found: {db_path}")
        print("Run parse_otel_metrics.py first to create the database.")
        return 1
    
    generate_report(str(db_path), str(output_path) if output_path else None)
    return 0


if __name__ == '__main__':
    exit(main())
```

### Script 3: generate_timeline.py

Save this as `scripts/generate_timeline.py`:

```python
#!/usr/bin/env python3
"""
Generate timeline visualization from local SQLite metrics database.

Usage:
    python generate_timeline.py [--db <database_file>] [--output <output_file>]
    
Example:
    python generate_timeline.py --db ~/claude_metrics.db --output timeline.html
"""

import sqlite3
import argparse
from datetime import datetime
from pathlib import Path


def get_timeline_data(cursor):
    """Get timeline data for all sessions."""
    cursor.execute('''
        SELECT 
            t.timestamp,
            t.session_id,
            t.token_type,
            t.value,
            t.model
        FROM token_usage t
        ORDER BY t.timestamp
    ''')
    
    return cursor.fetchall()


def get_cumulative_stats(cursor):
    """Get cumulative statistics over time."""
    cursor.execute('''
        SELECT 
            timestamp,
            token_type,
            SUM(value) OVER (PARTITION BY token_type ORDER BY timestamp) as cumulative
        FROM token_usage
        ORDER BY timestamp
    ''')
    
    return cursor.fetchall()


def generate_text_timeline(cursor):
    """Generate a text-based timeline."""
    timeline = []
    
    cursor.execute('''
        SELECT 
            t.timestamp,
            t.session_id,
            t.model,
            GROUP_CONCAT(t.token_type || ':' || t.value, ', ') as tokens
        FROM token_usage t
        GROUP BY t.timestamp, t.session_id, t.model
        ORDER BY t.timestamp
    ''')
    
    timeline.append("=" * 80)
    timeline.append("CLAUDE CODE METRICS TIMELINE")
    timeline.append("=" * 80)
    timeline.append("")
    
    current_session = None
    for row in cursor.fetchall():
        timestamp, session_id, model, tokens = row
        
        if session_id != current_session:
            current_session = session_id
            timeline.append(f"\n{'â”€' * 40}")
            timeline.append(f"SESSION: {session_id[:16]}...")
            timeline.append(f"MODEL: {model}")
            timeline.append(f"{'â”€' * 40}")
        
        timeline.append(f"  [{timestamp}] {tokens}")
    
    # Add summary
    cursor.execute('''
        SELECT 
            token_type,
            SUM(value) as total
        FROM token_usage
        GROUP BY token_type
    ''')
    
    timeline.append("\n" + "=" * 80)
    timeline.append("SUMMARY")
    timeline.append("=" * 80)
    for row in cursor.fetchall():
        timeline.append(f"  {row[0]}: {row[1]:,} tokens")
    
    return '\n'.join(timeline)


def generate_csv_timeline(cursor):
    """Generate CSV export of timeline data."""
    csv_lines = ['timestamp,session_id,model,token_type,value,cumulative']
    
    cursor.execute('''
        SELECT 
            t.timestamp,
            t.session_id,
            t.model,
            t.token_type,
            t.value,
            SUM(t.value) OVER (PARTITION BY t.token_type ORDER BY t.timestamp) as cumulative
        FROM token_usage t
        ORDER BY t.timestamp
    ''')
    
    for row in cursor.fetchall():
        csv_lines.append(','.join(str(x) for x in row))
    
    return '\n'.join(csv_lines)


def generate_html_timeline(cursor):
    """Generate an HTML timeline with basic visualization."""
    timeline_data = get_timeline_data(cursor)
    
    # Get totals for chart
    cursor.execute('''
        SELECT token_type, SUM(value) as total
        FROM token_usage
        GROUP BY token_type
    ''')
    totals = dict(cursor.fetchall())
    
    html = '''<!DOCTYPE html>
<html>
<head>
    <title>Claude Code Metrics Timeline</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; margin: 20px; }
        .header { text-align: center; margin-bottom: 30px; }
        .summary { display: flex; justify-content: space-around; margin-bottom: 30px; }
        .stat-box { background: #f5f5f5; padding: 20px; border-radius: 8px; text-align: center; }
        .stat-value { font-size: 24px; font-weight: bold; color: #333; }
        .stat-label { color: #666; margin-top: 5px; }
        .timeline { margin-top: 20px; }
        .timeline-item { display: flex; margin-bottom: 10px; padding: 10px; background: #fafafa; border-radius: 4px; }
        .timeline-time { width: 200px; color: #666; }
        .timeline-content { flex: 1; }
        .token-input { color: #2196F3; }
        .token-output { color: #4CAF50; }
        .token-cacheRead { color: #9C27B0; }
        .token-cacheCreation { color: #FF9800; }
        .bar-chart { margin-top: 30px; }
        .bar { height: 30px; margin-bottom: 5px; border-radius: 4px; display: flex; align-items: center; padding-left: 10px; color: white; }
        .bar-input { background: #2196F3; }
        .bar-output { background: #4CAF50; }
        .bar-cacheRead { background: #9C27B0; }
        .bar-cacheCreation { background: #FF9800; }
    </style>
</head>
<body>
    <div class="header">
        <h1>Claude Code Metrics Timeline</h1>
        <p>Generated: ''' + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + '''</p>
    </div>
    
    <div class="summary">
'''
    
    # Add summary boxes
    for token_type, total in totals.items():
        html += f'''
        <div class="stat-box">
            <div class="stat-value">{total:,}</div>
            <div class="stat-label">{token_type} tokens</div>
        </div>
'''
    
    html += '''
    </div>
    
    <h2>Token Distribution</h2>
    <div class="bar-chart">
'''
    
    # Add bar chart
    max_val = max(totals.values()) if totals else 1
    for token_type, total in totals.items():
        width = (total / max_val) * 100
        html += f'        <div class="bar bar-{token_type}" style="width: {width}%">{token_type}: {total:,}</div>\n'
    
    html += '''
    </div>
    
    <h2>Timeline</h2>
    <div class="timeline">
'''
    
    # Add timeline items (limit to last 100)
    for row in timeline_data[-100:]:
        timestamp, session_id, token_type, value, model = row
        html += f'''
        <div class="timeline-item">
            <div class="timeline-time">{timestamp}</div>
            <div class="timeline-content">
                <span class="token-{token_type}">{token_type}</span>: {value:,} tokens
                <small style="color: #999">({model})</small>
            </div>
        </div>
'''
    
    html += '''
    </div>
</body>
</html>
'''
    
    return html


def generate_timeline(db_path, output_path=None, format='text'):
    """Generate timeline from the database."""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    if format == 'html':
        content = generate_html_timeline(cursor)
    elif format == 'csv':
        content = generate_csv_timeline(cursor)
    else:
        content = generate_text_timeline(cursor)
    
    if output_path:
        with open(output_path, 'w') as f:
            f.write(content)
        print(f"Timeline saved to: {output_path}")
    else:
        print(content)
    
    conn.close()


def main():
    parser = argparse.ArgumentParser(
        description='Generate timeline visualization from local SQLite metrics database'
    )
    parser.add_argument('--db', default='~/.claude-metrics/metrics.db',
                        help='Path to SQLite database file')
    parser.add_argument('--output', '-o', help='Output file path (default: print to stdout)')
    parser.add_argument('--format', '-f', choices=['text', 'html', 'csv'], default='text',
                        help='Output format (default: text)')
    
    args = parser.parse_args()
    
    db_path = Path(args.db).expanduser()
    output_path = Path(args.output).expanduser() if args.output else None
    
    if not db_path.exists():
        print(f"Error: Database not found: {db_path}")
        print("Run parse_otel_metrics.py first to create the database.")
        return 1
    
    generate_timeline(str(db_path), str(output_path) if output_path else None, args.format)
    return 0


if __name__ == '__main__':
    exit(main())
```

---

## Database Schema

The SQLite database uses this schema:

```sql
-- Sessions table: Tracks unique Claude Code sessions
CREATE TABLE sessions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT UNIQUE,
    user_id TEXT,
    model TEXT,
    started_at TIMESTAMP,
    ended_at TIMESTAMP
);

-- Token usage: Tracks all token metrics
CREATE TABLE token_usage (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT,
    timestamp TIMESTAMP,
    model TEXT,
    token_type TEXT,  -- 'input', 'output', 'cacheRead', 'cacheCreation'
    value INTEGER
);

-- Cost usage: Tracks cost in USD
CREATE TABLE cost_usage (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT,
    timestamp TIMESTAMP,
    model TEXT,
    value REAL
);

-- Active time: Tracks session duration
CREATE TABLE active_time (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT,
    timestamp TIMESTAMP,
    value REAL  -- seconds
);

-- Code activity: Tracks commits, PRs, lines of code
CREATE TABLE code_activity (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT,
    timestamp TIMESTAMP,
    activity_type TEXT,  -- 'commit', 'pull_request', 'lines_of_code'
    value INTEGER
);
```

### Useful SQL Queries

```sql
-- Total tokens by type
SELECT token_type, SUM(value) as total
FROM token_usage
GROUP BY token_type;

-- Cache hit ratio
SELECT 
    SUM(CASE WHEN token_type = 'cacheRead' THEN value ELSE 0 END) * 1.0 /
    (SUM(CASE WHEN token_type = 'cacheRead' THEN value ELSE 0 END) +
     SUM(CASE WHEN token_type = 'cacheCreation' THEN value ELSE 0 END)) as cache_hit_ratio
FROM token_usage;

-- Total cost
SELECT SUM(value) as total_cost FROM cost_usage;

-- Tokens per session
SELECT 
    session_id,
    SUM(CASE WHEN token_type = 'input' THEN value ELSE 0 END) as input_tokens,
    SUM(CASE WHEN token_type = 'output' THEN value ELSE 0 END) as output_tokens
FROM token_usage
GROUP BY session_id;

-- Daily token usage
SELECT 
    DATE(timestamp) as date,
    token_type,
    SUM(value) as tokens
FROM token_usage
GROUP BY DATE(timestamp), token_type
ORDER BY date;
```

---

## Generating Reports

### Basic Report

```bash
python scripts/generate_local_report.py --db ~/.claude-metrics/metrics.db
```

### Save to File

```bash
python scripts/generate_local_report.py --db ~/.claude-metrics/metrics.db --output report.md
```

### Sample Report Output

```markdown
# Claude Code Local Metrics Report

**Generated**: 2025-01-15 10:30:00

## Summary

- **Total Sessions**: 5
- **Total Cost**: $0.4523
- **Total Active Time**: 1234.5 seconds (20.6 minutes)

## Token Usage

| Type | Count |
|------|-------|
| input | 2,500 |
| output | 3,200 |
| cacheRead | 45,000 |
| cacheCreation | 1,200 |
| **Total** | **51,900** |

## Cache Efficiency

- **Cache Hit Ratio**: 97.4%
- **Cache Efficiency**: 94.7%
- **Cache Read:Creation Ratio**: 37.5:1
- **Estimated Cost Savings**: $0.1215
```

---

## Timeline Visualization

### Text Timeline

```bash
python scripts/generate_timeline.py --db ~/.claude-metrics/metrics.db
```

### HTML Timeline

```bash
python scripts/generate_timeline.py --db ~/.claude-metrics/metrics.db --format html --output timeline.html
```

### CSV Export

```bash
python scripts/generate_timeline.py --db ~/.claude-metrics/metrics.db --format csv --output timeline.csv
```

---

## Combining with Session JSONL Files

The local session JSONL files at `~/.claude/projects/<session>.jsonl` contain additional information that can complement OTEL metrics. Here's a script to combine both sources:

### Script: combine_sources.py

```python
#!/usr/bin/env python3
"""
Combine OTEL metrics with session JSONL files for comprehensive analysis.
"""

import json
import sqlite3
from pathlib import Path
from datetime import datetime


def parse_session_jsonl(jsonl_path):
    """Parse a session JSONL file."""
    entries = []
    with open(jsonl_path, 'r') as f:
        for line in f:
            if line.strip():
                try:
                    entries.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
    return entries


def find_session_files():
    """Find all session JSONL files."""
    claude_dir = Path.home() / '.claude' / 'projects'
    if not claude_dir.exists():
        return []
    
    return list(claude_dir.glob('**/*.jsonl'))


def merge_data(db_path):
    """Merge OTEL metrics with session JSONL data."""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # Add table for JSONL data if not exists
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS session_messages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            session_id TEXT,
            timestamp TIMESTAMP,
            role TEXT,
            content_preview TEXT,
            tool_use TEXT
        )
    ''')
    
    session_files = find_session_files()
    
    for jsonl_file in session_files:
        session_id = jsonl_file.stem
        entries = parse_session_jsonl(jsonl_file)
        
        for entry in entries:
            role = entry.get('role', 'unknown')
            content = entry.get('content', '')
            if isinstance(content, list):
                content = str(content[0]) if content else ''
            content_preview = content[:200] if content else ''
            
            cursor.execute('''
                INSERT INTO session_messages (session_id, role, content_preview)
                VALUES (?, ?, ?)
            ''', (session_id, role, content_preview))
    
    conn.commit()
    conn.close()
    print(f"Merged {len(session_files)} session files")


if __name__ == '__main__':
    import sys
    db_path = sys.argv[1] if len(sys.argv) > 1 else '~/.claude-metrics/metrics.db'
    merge_data(Path(db_path).expanduser())
```

---

## Quick Reference

### Environment Variables for Offline Mode

```bash
# Required
export CLAUDE_CODE_ENABLE_TELEMETRY=1
export OTEL_METRICS_EXPORTER=console

# Recommended
export OTEL_METRIC_EXPORT_INTERVAL=5000  # 5 seconds
```

### Workflow Commands

```bash
# 1. Capture metrics
claude 2>&1 | tee -a ~/claude_metrics.log

# 2. Parse and store
python scripts/parse_otel_metrics.py ~/claude_metrics.log

# 3. Generate report
python scripts/generate_local_report.py --output report.md

# 4. View timeline
python scripts/generate_timeline.py --format html --output timeline.html
```

### Database Location

Default: `~/.claude-metrics/metrics.db`

### Files Created

| File | Purpose |
|------|---------|
| `~/.claude-metrics/metrics.db` | SQLite database with parsed metrics |
| `~/claude_metrics.log` | Raw OTEL console output |
| `report.md` | Generated markdown report |
| `timeline.html` | HTML timeline visualization |
| `timeline.csv` | CSV export for spreadsheet analysis |
